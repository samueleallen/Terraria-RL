{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca656786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe537f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64*5*5, 64)\n",
    "        self.fc2 = nn.Linear(64, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88ce5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec23f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1), shear=0.1, scale=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(brightness=0.3),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, X, y, augmentation_factor=50, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (numpy array): Training images, shape of (N, 28, 28)\n",
    "            y (numpy array): The corresponding label of each image, shape of (N,)\n",
    "            augmentation_factor: Number of augmented versions to create per image\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.augmentation_factor = augmentation_factor\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Pre-generates and stores all augmented samples\n",
    "        self.samples = self._augment_samples()\n",
    "\n",
    "    def _augment_samples(self):\n",
    "        \"\"\"\n",
    "        Creates a new dataset consisting of both orignal images and augmented images\n",
    "        Returns:\n",
    "            list of tuples (image, label)\n",
    "        \"\"\"\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "\n",
    "        # Add original dataset\n",
    "        for img, label in zip(self.X, self.y):\n",
    "            img = img.reshape(28, 28)\n",
    "            img_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "            augmented_images.append(img_tensor)\n",
    "            augmented_labels.append(label)\n",
    "\n",
    "        # Add augmented dataset\n",
    "        for img, label in zip(self.X, self.y):\n",
    "            img = img.reshape(28, 28)\n",
    "\n",
    "            for i in range(self.augmentation_factor):\n",
    "                if self.transform:\n",
    "                    # Apply random transformation\n",
    "                    aug_img = self.transform(img)\n",
    "\n",
    "                    augmented_images.append(aug_img)\n",
    "                    augmented_labels.append(label)\n",
    "\n",
    "        # Zip back into a list of image-label tuples\n",
    "        return list(zip(augmented_images, augmented_labels))\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returns total number of samples (including augmented)\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Gets a sample based on index given and converts img-label to pytorch tensors\n",
    "        img, label = self.samples[index]\n",
    "\n",
    "        # Convert img to float tensor and add channel dimension if necessary\n",
    "        img_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "        if img_tensor.ndim == 2: # if still only [H, W], add channel\n",
    "            img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "        # Convert label to integer tensor\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return img_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cf713fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_images(folder_path):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            # Load image\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = Image.open(img_path).convert('L') # Convert img to grayscale\n",
    "            img = img.resize((28, 28))\n",
    "            img_array = np.array(img) / 255.0 # Normalize\n",
    "            img_array = img_array.reshape(28, 28, 1) # Add channel dimension\n",
    "\n",
    "            # Extract label from filename (Assumes filename starts with a number)\n",
    "            if filename.startswith('slash'):\n",
    "                label = 10 # Assings '/' to be encoded as 10\n",
    "            else:\n",
    "                label = int(filename[0])\n",
    "            \n",
    "            images.append(img_array)\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5698b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 693\n",
      "Val size: 16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from image folder\n",
    "folder_path = \"C:/Users/Sam/Documents/Comp Sci/Terraria Bot/Terraria-Bot/dataset/Health Numbers/Resized_digits\"\n",
    "X, y = load_images(folder_path)\n",
    "\n",
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split train into train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train if len(set(y_train)) > 1 else None)\n",
    "\n",
    "# Assumes X_train and y_train are numpy arrays\n",
    "train_dataset = AugmentedDataset(X_train, y_train, augmentation_factor=10, transform=transform)\n",
    "val_dataset = AugmentedDataset(X_val, y_val, augmentation_factor=0, transform=transform)\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21daf318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_17176\\3122656265.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_tensor = torch.tensor(img, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.3880, Train Acc: 13.28%, Val Loss: 2.3687, Val Acc: 12.50%\n",
      "Epoch 2, Train Loss: 2.3329, Train Acc: 14.14%, Val Loss: 2.2541, Val Acc: 25.00%\n",
      "Epoch 3, Train Loss: 2.1660, Train Acc: 21.50%, Val Loss: 1.9149, Val Acc: 31.25%\n",
      "Epoch 4, Train Loss: 1.7928, Train Acc: 39.11%, Val Loss: 1.3989, Val Acc: 50.00%\n",
      "Epoch 5, Train Loss: 1.3792, Train Acc: 53.25%, Val Loss: 1.1644, Val Acc: 68.75%\n",
      "Epoch 6, Train Loss: 1.2204, Train Acc: 58.30%, Val Loss: 1.0248, Val Acc: 68.75%\n",
      "Epoch 7, Train Loss: 1.0089, Train Acc: 66.38%, Val Loss: 0.9941, Val Acc: 68.75%\n",
      "Epoch 8, Train Loss: 0.9782, Train Acc: 65.51%, Val Loss: 0.8279, Val Acc: 87.50%\n",
      "Epoch 9, Train Loss: 0.8374, Train Acc: 73.02%, Val Loss: 0.7302, Val Acc: 87.50%\n",
      "Epoch 10, Train Loss: 0.7748, Train Acc: 74.75%, Val Loss: 0.7230, Val Acc: 87.50%\n",
      "Epoch 11, Train Loss: 0.7503, Train Acc: 76.05%, Val Loss: 0.6356, Val Acc: 87.50%\n",
      "Epoch 12, Train Loss: 0.7355, Train Acc: 75.32%, Val Loss: 0.8466, Val Acc: 81.25%\n",
      "Epoch 13, Train Loss: 0.6764, Train Acc: 79.51%, Val Loss: 0.5900, Val Acc: 87.50%\n",
      "Epoch 14, Train Loss: 0.5955, Train Acc: 81.24%, Val Loss: 0.5863, Val Acc: 87.50%\n",
      "Epoch 15, Train Loss: 0.5925, Train Acc: 81.53%, Val Loss: 0.5647, Val Acc: 87.50%\n",
      "Epoch 16, Train Loss: 0.5893, Train Acc: 81.96%, Val Loss: 0.8471, Val Acc: 81.25%\n",
      "Epoch 17, Train Loss: 0.5802, Train Acc: 80.81%, Val Loss: 0.5806, Val Acc: 87.50%\n",
      "Epoch 18, Train Loss: 0.5230, Train Acc: 83.69%, Val Loss: 0.7570, Val Acc: 87.50%\n",
      "Epoch 19, Train Loss: 0.5096, Train Acc: 83.84%, Val Loss: 0.4844, Val Acc: 87.50%\n",
      "Epoch 20, Train Loss: 0.4693, Train Acc: 85.86%, Val Loss: 0.6459, Val Acc: 87.50%\n",
      "Epoch 21, Train Loss: 0.4674, Train Acc: 84.99%, Val Loss: 0.7007, Val Acc: 87.50%\n",
      "Epoch 22, Train Loss: 0.4157, Train Acc: 87.59%, Val Loss: 0.4943, Val Acc: 87.50%\n",
      "Epoch 23, Train Loss: 0.4166, Train Acc: 87.01%, Val Loss: 0.6659, Val Acc: 87.50%\n",
      "Epoch 24, Train Loss: 0.3955, Train Acc: 89.61%, Val Loss: 0.6266, Val Acc: 87.50%\n",
      "Epoch 25, Train Loss: 0.3514, Train Acc: 89.47%, Val Loss: 0.5186, Val Acc: 87.50%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(25):\n",
    "    train_loss, train_total, train_correct = 0, 0, 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        i, prediced = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (prediced==labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "\n",
    "    # Model Validation\n",
    "    model.eval()\n",
    "    val_correct, val_total, val_loss = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            i, prediced = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (prediced==labels).sum().item()\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, \" \n",
    "          f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "          f\"Train Acc: {train_acc:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317aaeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"text_classifier_weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
