{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca656786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe537f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(64*5*5, 64)\n",
    "        self.fc1_dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(64, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 64*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88ce5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ec23f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1), shear=0.1, scale=(0.9, 1.1)),\n",
    "    transforms.ColorJitter(brightness=0.3),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, X, y, augmentation_factor=50, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (numpy array): Training images, shape of (N, 28, 28)\n",
    "            y (numpy array): The corresponding label of each image, shape of (N,)\n",
    "            augmentation_factor: Number of augmented versions to create per image\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.augmentation_factor = augmentation_factor\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Pre-generates and stores all augmented samples\n",
    "        self.samples = self._augment_samples()\n",
    "\n",
    "    def _augment_samples(self):\n",
    "        \"\"\"\n",
    "        Creates a new dataset consisting of both orignal images and augmented images\n",
    "        Returns:\n",
    "            list of tuples (image, label)\n",
    "        \"\"\"\n",
    "        augmented_images = []\n",
    "        augmented_labels = []\n",
    "\n",
    "        # Add original dataset\n",
    "        for img, label in zip(self.X, self.y):\n",
    "            img = img.reshape(28, 28)\n",
    "            img_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "            augmented_images.append(img_tensor)\n",
    "            augmented_labels.append(label)\n",
    "\n",
    "        # Add augmented dataset\n",
    "        for img, label in zip(self.X, self.y):\n",
    "            img = img.reshape(28, 28)\n",
    "\n",
    "            for i in range(self.augmentation_factor):\n",
    "                if self.transform:\n",
    "                    # Apply random transformation\n",
    "                    aug_img = self.transform(img)\n",
    "\n",
    "                    augmented_images.append(aug_img)\n",
    "                    augmented_labels.append(label)\n",
    "\n",
    "        # Zip back into a list of image-label tuples\n",
    "        return list(zip(augmented_images, augmented_labels))\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Returns total number of samples (including augmented)\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Gets a sample based on index given and converts img-label to pytorch tensors\n",
    "        img, label = self.samples[index]\n",
    "\n",
    "        # Convert img to float tensor and add channel dimension if necessary\n",
    "        img_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "        if img_tensor.ndim == 2: # if still only [H, W], add channel\n",
    "            img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "        # Convert label to integer tensor\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return img_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cf713fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_images(folder_path):\n",
    "    images=[]\n",
    "    labels=[]\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            # Load image\n",
    "            img_path = os.path.join(folder_path, filename)\n",
    "            img = Image.open(img_path).convert('L') # Convert img to grayscale\n",
    "            img = img.resize((28, 28))\n",
    "            img_array = np.array(img) / 255.0 # Normalize\n",
    "            img_array = img_array.reshape(28, 28, 1) # Add channel dimension\n",
    "\n",
    "            # Extract label from filename (Assumes filename starts with a number)\n",
    "            if filename.startswith('slash'):\n",
    "                label = 10 # Assings '/' to be encoded as 10\n",
    "            else:\n",
    "                label = int(filename[0])\n",
    "            \n",
    "            images.append(img_array)\n",
    "            labels.append(label)\n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5698b565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 693\n",
      "Val size: 16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from image folder\n",
    "folder_path = \"C:/Users/Sam/Documents/Comp Sci/Terraria Bot/Terraria-Bot/dataset/Health Numbers/Resized_digits\"\n",
    "X, y = load_images(folder_path)\n",
    "\n",
    "# Split into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Further split train into train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train if len(set(y_train)) > 1 else None)\n",
    "\n",
    "# Assumes X_train and y_train are numpy arrays\n",
    "train_dataset = AugmentedDataset(X_train, y_train, augmentation_factor=10, transform=transform)\n",
    "val_dataset = AugmentedDataset(X_val, y_val, augmentation_factor=0, transform=transform)\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNModel().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21daf318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sam\\AppData\\Local\\Temp\\ipykernel_14452\\1176996452.py:65: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  img_tensor = torch.tensor(img, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 2.3998, Train Acc: 10.39%, Val Loss: 2.3936, Val Acc: 12.50%\n",
      "Epoch 2, Train Loss: 2.3672, Train Acc: 12.41%, Val Loss: 2.3248, Val Acc: 12.50%\n",
      "Epoch 3, Train Loss: 2.2955, Train Acc: 16.16%, Val Loss: 2.1983, Val Acc: 18.75%\n",
      "Epoch 4, Train Loss: 2.1386, Train Acc: 23.81%, Val Loss: 1.9006, Val Acc: 43.75%\n",
      "Epoch 5, Train Loss: 1.8759, Train Acc: 36.80%, Val Loss: 1.4696, Val Acc: 62.50%\n",
      "Epoch 6, Train Loss: 1.5627, Train Acc: 49.21%, Val Loss: 1.3232, Val Acc: 62.50%\n",
      "Epoch 7, Train Loss: 1.3302, Train Acc: 54.98%, Val Loss: 1.1103, Val Acc: 75.00%\n",
      "Epoch 8, Train Loss: 1.1517, Train Acc: 63.49%, Val Loss: 1.0350, Val Acc: 68.75%\n",
      "Epoch 9, Train Loss: 1.0001, Train Acc: 68.69%, Val Loss: 0.9738, Val Acc: 75.00%\n",
      "Epoch 10, Train Loss: 0.9583, Train Acc: 70.56%, Val Loss: 0.9801, Val Acc: 75.00%\n",
      "Epoch 11, Train Loss: 0.8829, Train Acc: 71.57%, Val Loss: 0.8148, Val Acc: 87.50%\n",
      "Epoch 12, Train Loss: 0.7784, Train Acc: 76.91%, Val Loss: 0.9043, Val Acc: 81.25%\n",
      "Epoch 13, Train Loss: 0.7303, Train Acc: 76.48%, Val Loss: 0.7722, Val Acc: 81.25%\n",
      "Epoch 14, Train Loss: 0.6976, Train Acc: 78.79%, Val Loss: 0.7914, Val Acc: 87.50%\n",
      "Epoch 15, Train Loss: 0.6453, Train Acc: 80.66%, Val Loss: 0.8803, Val Acc: 81.25%\n",
      "Epoch 16, Train Loss: 0.5625, Train Acc: 83.84%, Val Loss: 1.0354, Val Acc: 75.00%\n",
      "Epoch 17, Train Loss: 0.6387, Train Acc: 80.09%, Val Loss: 0.9017, Val Acc: 81.25%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(17):\n",
    "    train_loss, train_total, train_correct = 0, 0, 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        i, prediced = torch.max(outputs, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (prediced==labels).sum().item()\n",
    "\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "\n",
    "    # Model Validation\n",
    "    model.eval()\n",
    "    val_correct, val_total, val_loss = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            i, prediced = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (prediced==labels).sum().item()\n",
    "    \n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, \" \n",
    "          f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "          f\"Train Acc: {train_acc:.2f}%, \"\n",
    "          f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "317aaeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"text_classifier_weights.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
